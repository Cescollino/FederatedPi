
@misc{noauthor_differential_2021,
	title = {Differential privacy},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Differential_privacy&oldid=1061496127},
	abstract = {Differential privacy (DP) is a system for publicly sharing information about a dataset by describing the patterns of groups within the dataset while withholding information about individuals in the dataset. The idea behind differential privacy is that if the effect of making an arbitrary single substitution in the database is small enough, the query result cannot be used to infer much about any single individual, and therefore provides privacy. Another way to describe differential privacy is as a constraint on the algorithms used to publish aggregate information about a statistical database which limits the disclosure of private information of records whose information is in the database. For example, differentially private algorithms are used by some government agencies to publish demographic information or other statistical aggregates while ensuring confidentiality of survey responses, and by companies to collect information about user behavior while controlling what is visible even to internal analysts.
Roughly, an algorithm is differentially private if an observer seeing its output cannot tell if a particular individual's information was used in the computation. 
Differential privacy is often discussed in the context of identifying individuals whose information may be in a database. Although it does not directly refer to identification and reidentification attacks, differentially private algorithms probably resist such attacks.Differential privacy was developed by cryptographers and thus is often associated with cryptography, and draws much of its language from cryptography.},
	language = {en},
	urldate = {2022-02-02},
	journal = {Wikipedia},
	month = dec,
	year = {2021},
	note = {Page Version ID: 1061496127},
	file = {Snapshot:C\:\\Users\\Cescollino\\Zotero\\storage\\UVZGTSCT\\Differential_privacy.html:text/html},
}

@article{mcmahan_communication-efficient_2017,
	title = {Communication-{Efficient} {Learning} of {Deep} {Networks} from {Decentralized} {Data}},
	url = {http://arxiv.org/abs/1602.05629},
	abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
	urldate = {2022-02-02},
	journal = {arXiv:1602.05629 [cs]},
	author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Ag√ºera y},
	month = feb,
	year = {2017},
	note = {arXiv: 1602.05629},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Cescollino\\Zotero\\storage\\PVMZ2F5C\\McMahan et al. - 2017 - Communication-Efficient Learning of Deep Networks .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Cescollino\\Zotero\\storage\\GTQ5Z5EF\\1602.html:text/html},
}

@misc{jadhav_federated-learning_2022,
	title = {Federated-{Learning} ({PyTorch})},
	copyright = {MIT},
	url = {https://github.com/AshwinRJ/Federated-Learning-PyTorch},
	abstract = {Implementation of Communication-Efficient Learning of Deep Networks from Decentralized Data},
	urldate = {2022-02-02},
	author = {Jadhav, Ashwin R.},
	month = feb,
	year = {2022},
	note = {original-date: 2018-11-16T23:51:14Z},
	keywords = {deep-learning, distributed-computing, federated-learning, python, pytorch},
}
